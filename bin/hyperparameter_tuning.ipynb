{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaushouben\u001b[0m (\u001b[33mwattcast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\nik\\Desktop\\Berkeley_Projects\\WattCast\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "import darts\n",
    "from darts.dataprocessing.transformers.boxcox import BoxCox\n",
    "from darts.models import LightGBMModel, XGBModel, LinearRegressionModel, NBEATSModel, BlockRNNModel, RandomForest\n",
    "from darts.metrics import smape, mape, mase, mse, rmse, r2_score, mae\n",
    "from darts.dataprocessing.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler   \n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from darts.utils.missing_values import extract_subseries\n",
    "\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "from wandb.xgboost import WandbCallback\n",
    "\n",
    "\n",
    "from utils import *\n",
    "from train_eval import *\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(r\"..\") # should be the git repo root directory, checking below:\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "assert os.getcwd()[-8:] == \"WattCast\"\n",
    "dir_path = os.path.join(os.getcwd(), 'data', 'clean_data')\n",
    "model_dir = os.path.join(os.getcwd(), 'models')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_eval_light():\n",
    "\n",
    "    wandb.init(project=\"WattCast_tuning\")\n",
    "    wandb.config.update(config_run)\n",
    "    config = wandb.config\n",
    "\n",
    "    print(\"Getting data...\")\n",
    "\n",
    "    pipeline, ts_train_piped, ts_val_piped, ts_test_piped, ts_train_weather_piped, ts_val_weather_piped, ts_test_weather_piped, trg_train_inversed, trg_val_inversed, trg_test_inversed = data_pipeline(config)\n",
    "\n",
    "    print(\"Getting model instance...\")\n",
    "    model = get_model(config)\n",
    "    model, runtime = train_models([model], ts_train_piped, ts_train_weather_piped, ts_val_piped, ts_val_weather_piped)\n",
    "\n",
    "    print(\"Evaluating model...\")\n",
    "    predictions, score = predict_testset(model[0], \n",
    "                                  ts_test_piped[config.longest_ts_test_idx], \n",
    "                                  ts_test_weather_piped[config.longest_ts_test_idx],\n",
    "                                  config.n_lags, config.n_ahead, config.eval_stride, pipeline,\n",
    "                                  )\n",
    "\n",
    "\n",
    "    print(\"Plotting predictions...\")\n",
    "    df_compare = pd.concat([trg_test_inversed.pd_dataframe(), predictions], axis=1).dropna()\n",
    "    df_compare.columns = ['target', 'prediction']\n",
    "    fig = px.line(df_compare, title='Predictions vs. Test Set')\n",
    "\n",
    "    wandb.log({'eval_loss': score})\n",
    "    wandb.log({'predictions': fig})\n",
    "    wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what keys are in the h5py data file\n",
    "get_hdf_keys(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8hhstzpo\n",
      "Sweep URL: https://wandb.ai/wattcast/Wattcast_tuning/sweeps/8hhstzpo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 87ikdfka with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdatetime_encodings: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 1024\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tn_rnn_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_attention_heads: 6\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4a328b166945e5a958f960914e16f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nik\\Desktop\\Berkeley_Projects\\WattCast\\wandb\\run-20230622_165230-87ikdfka</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wattcast/Wattcast_tuning/runs/87ikdfka' target=\"_blank\">upbeat-sweep-1</a></strong> to <a href='https://wandb.ai/wattcast/Wattcast_tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/wattcast/Wattcast_tuning/sweeps/8hhstzpo' target=\"_blank\">https://wandb.ai/wattcast/Wattcast_tuning/sweeps/8hhstzpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wattcast/Wattcast_tuning' target=\"_blank\">https://wandb.ai/wattcast/Wattcast_tuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/wattcast/Wattcast_tuning/sweeps/8hhstzpo' target=\"_blank\">https://wandb.ai/wattcast/Wattcast_tuning/sweeps/8hhstzpo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wattcast/Wattcast_tuning/runs/87ikdfka' target=\"_blank\">https://wandb.ai/wattcast/Wattcast_tuning/runs/87ikdfka</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'datetime_encodings' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data...\n",
      "Getting model instance...\n",
      "Training TFTModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 74.3 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 49.5 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 4.2 M \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 4.2 M \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 4.2 M \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 4.2 M \n",
      "10 | lstm_encoder                      | LSTM                             | 8.4 M \n",
      "11 | lstm_decoder                      | LSTM                             | 8.4 M \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 2.1 M \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 5.2 M \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 2.4 M \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 2.1 M \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 4.2 M \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 2.1 M \n",
      "18 | output_layer                      | Linear                           | 17.4 K\n",
      "----------------------------------------------------------------------------------------\n",
      "51.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "51.9 M    Total params\n",
      "415.434   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9f9dc4417a4608a7ab007550ab42c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84f391041bd4a55a59b7bd63f754c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run parameters\n",
    "\n",
    "sweeps = 20\n",
    "\n",
    "scale_location_pairs = (\n",
    "    ('4_neighborhood', 'germany'),\n",
    "     #('5_household', 'household_0'),\n",
    "      )\n",
    "\n",
    "\n",
    "\n",
    "models = [\n",
    "        # 'rf',\n",
    "        # 'xgb', \n",
    "        # 'gru', \n",
    "        # 'lgbm',  \n",
    "        # 'nbeats',\n",
    "        'transformer'\n",
    "\n",
    "\n",
    "        ]\n",
    "\n",
    "for scale, location in scale_location_pairs:\n",
    "\n",
    "    for model in models:\n",
    "\n",
    "        config_run = {\n",
    "            'spatial_scale': scale,\n",
    "            'temp_resolution': 60,\n",
    "            'location': location,\n",
    "            'model': model,\n",
    "            'horizon_in_hours': 24,\n",
    "            'lookback_in_hours': 24,\n",
    "            'boxcox': True,\n",
    "            'liklihood': None,\n",
    "            'weather': True,\n",
    "            'holiday': True,\n",
    "            'datetime_encodings': False,\n",
    "        }\n",
    "\n",
    "        with open(f'sweep_configurations/config_sweep_{model}.json', 'r') as fp:\n",
    "            sweep_config = json.load(fp)                  \n",
    "\n",
    "        sweep_config['name'] = model + 'sweep' + config_run['spatial_scale'] + '_' + config_run['location'] + '_' + str(config_run['temp_resolution'])\n",
    "\n",
    "        sweep_id = wandb.sweep(sweep_config, project=\"WattCast_tuning\")\n",
    "        wandb.agent(sweep_id, train_eval_light, count=sweeps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
