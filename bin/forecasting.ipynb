{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaushouben\u001b[0m (\u001b[33mwattcast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import darts\n",
    "from darts.utils.statistics import check_seasonality, plot_acf, stationarity_tests\n",
    "from darts.dataprocessing.transformers.missing_values_filler import MissingValuesFiller\n",
    "from darts.dataprocessing.transformers.boxcox import BoxCox\n",
    "from darts.dataprocessing.transformers.diff import Diff\n",
    "from darts.utils.statistics import plot_hist\n",
    "from darts.models import LightGBMModel, XGBModel, LinearRegressionModel, TFTModel, NHiTSModel\n",
    "from darts.metrics import smape, mape, mase, mse, rmse, r2_score, mae\n",
    "from darts.dataprocessing.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler   \n",
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "\n",
    "from utils import *\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\nik\\Desktop\\Berkeley_Projects\\WattCast\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "os.chdir(r\"..\") # should be the git repo root directory, checking below:\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "assert os.getcwd()[-8:] == \"WattCast\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run parameters\n",
    "\n",
    "dir_path = os.path.join(os.getcwd(), 'data', 'clean_data')\n",
    "\n",
    "\n",
    "config_dict = {\n",
    "    'spatial_scale': 'county',\n",
    "    'temp_resolution': '60',\n",
    "    'location': 'New_York',\n",
    "    'horizon': 24, # in hours\n",
    "    'n_lags': 24, # timesteps lookback\n",
    "}\n",
    "    \n",
    "spatial_scale = 'county'\n",
    "temp_resolution = '60'\n",
    "location = 'New_York'\n",
    "horizon = 24 # in hours\n",
    "\n",
    "\n",
    "n_lags = 24 # timesteps lookback\n",
    "n_ahead = int(horizon * 60//int(temp_resolution)) # timesteps ahead\n",
    "\n",
    "list_sklearn_models= [\n",
    "                        LinearRegressionModel,\n",
    "                        LightGBMModel,\n",
    "                        XGBModel,\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File /Users/nikolaushouben/Desktop/WattCast/bin/data/clean_data/county_data.h5 does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Loading Data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_train \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_hdf(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(dir_path, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mspatial_scale\u001b[39m}\u001b[39;49;00m\u001b[39m_data.h5\u001b[39;49m\u001b[39m'\u001b[39;49m), key\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mtemp_resolution\u001b[39m}\u001b[39;49;00m\u001b[39mmin/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlocation\u001b[39m}\u001b[39;49;00m\u001b[39m/train\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m df_val \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_hdf(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dir_path, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mspatial_scale\u001b[39m}\u001b[39;00m\u001b[39m_data.h5\u001b[39m\u001b[39m'\u001b[39m), key\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtemp_resolution\u001b[39m}\u001b[39;00m\u001b[39mmin/\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m/val\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m df_test \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_hdf(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dir_path, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mspatial_scale\u001b[39m}\u001b[39;00m\u001b[39m_data.h5\u001b[39m\u001b[39m'\u001b[39m), key\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mtemp_resolution\u001b[39m}\u001b[39;00m\u001b[39mmin/\u001b[39m\u001b[39m{\u001b[39;00mlocation\u001b[39m}\u001b[39;00m\u001b[39m/test\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/Wattcast/lib/python3.11/site-packages/pandas/io/pytables.py:414\u001b[0m, in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, errors, where, start, stop, columns, iterator, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     exists \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m exists:\n\u001b[0;32m--> 414\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mpath_or_buf\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    416\u001b[0m store \u001b[39m=\u001b[39m HDFStore(path_or_buf, mode\u001b[39m=\u001b[39mmode, errors\u001b[39m=\u001b[39merrors, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    417\u001b[0m \u001b[39m# can't auto open/close if we are using an iterator\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[39m# so delegate to the iterator\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File /Users/nikolaushouben/Desktop/WattCast/bin/data/clean_data/county_data.h5 does not exist"
     ]
    }
   ],
   "source": [
    "# Loading Data\n",
    "df_train = pd.read_hdf(os.path.join(dir_path, f'{spatial_scale}_data.h5'), key=f'{temp_resolution}min/{location}/train')\n",
    "df_val = pd.read_hdf(os.path.join(dir_path, f'{spatial_scale}_data.h5'), key=f'{temp_resolution}min/{location}/val')\n",
    "df_test = pd.read_hdf(os.path.join(dir_path, f'{spatial_scale}_data.h5'), key=f'{temp_resolution}min/{location}/test')\n",
    "df_cov_train = pd.read_hdf(os.path.join(dir_path, f'{spatial_scale}_data.h5'), key=f'{temp_resolution}min/{location}/train_weather')\n",
    "df_cov_val = pd.read_hdf(os.path.join(dir_path, f'{spatial_scale}_data.h5'), key=f'{temp_resolution}min/{location}/val_weather')\n",
    "df_cov_test = pd.read_hdf(os.path.join(dir_path, f'{spatial_scale}_data.h5'), key=f'{temp_resolution}min/{location}/test_weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to darts format\n",
    "\n",
    "# into darts format\n",
    "\n",
    "ts_train = darts.TimeSeries.from_dataframe(df_train)\n",
    "ts_val = darts.TimeSeries.from_dataframe(df_val)\n",
    "ts_test = darts.TimeSeries.from_dataframe(df_test)\n",
    "\n",
    "# Covariates\n",
    "ts_cov_train = darts.TimeSeries.from_dataframe(df_cov_train)\n",
    "ts_cov_val = darts.TimeSeries.from_dataframe(df_cov_val)\n",
    "ts_cov_test = darts.TimeSeries.from_dataframe(df_cov_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "\n",
    "# Target pipeline\n",
    "pipeline = Pipeline(\n",
    "                    [\n",
    "                    MissingValuesFiller(), \n",
    "                    Scaler(MinMaxScaler()),\n",
    "                    ]\n",
    "                     )\n",
    "\n",
    "\n",
    "ts_train_piped = pipeline.fit_transform(ts_train)\n",
    "ts_val_piped = pipeline.transform(ts_val)\n",
    "ts_test_piped = pipeline.transform(ts_test)\n",
    "\n",
    "# Weather Pipeline\n",
    "pipeline_weather = Pipeline([MissingValuesFiller() ,Scaler(RobustScaler())])\n",
    "ts_train_weather_piped = pipeline_weather.fit_transform(ts_cov_train)\n",
    "ts_val_weather_piped = pipeline_weather.transform(ts_cov_val)\n",
    "ts_test_weather_piped = pipeline_weather.transform(ts_cov_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model settings \n",
    "\n",
    "\n",
    "trg_train, covs_train = make_index_same(ts_train_piped, ts_train_weather_piped)\n",
    "trg_train_inversed = pipeline.inverse_transform(trg_train, partial=True) # inverse transform the target, we need the original values for the evaluation\n",
    "\n",
    "trg_test, covs_test = make_index_same(ts_test_piped, ts_test_weather_piped)\n",
    "trg_test_inversed = pipeline.inverse_transform(trg_test, partial=True) # inverse transform the target, we need the original values for the evaluation\n",
    "\n",
    "encoders = {\n",
    "            \"cyclic\": {\"future\": [\"hour\"]}, \n",
    "            \"position\": {\"future\": [\"relative\",]},\n",
    "            \"datetime_attribute\": {\"future\": [\"dayofweek\", \"week\"]}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate models\n",
    "liklihood = None\n",
    "\n",
    "models_with_weather = make_sklearn_models(list_sklearn_models, encoders, n_lags, n_ahead, liklihood)\n",
    "\n",
    "# Train the models with weather\n",
    "models_with_weather = train_models(models_with_weather, \n",
    "                        trg_train, \n",
    "                        covs_train\n",
    "                        )\n",
    "model_names_with_weather = [model.__class__.__name__ + \"_with_weather\" for model in models_with_weather]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = models_with_weather\n",
    "model_names = model_names_with_weather\n",
    "models_dict = {model_name: model for model_name, model in zip(model_names, models)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c580f530964783862448f69c728234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658d86e3d90d4397a73a896bcb5fe216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e19e5c54722459380516fa74c21e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5585 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating Historical Forecasts for each model\n",
    "ts_predictions_per_model = {}\n",
    "historics_per_model = {}\n",
    "\n",
    "for model_name, model in models_dict.items():\n",
    "    covs_use = None if model_name.endswith(\"o_weather\") else covs_test\n",
    "    historics = model.historical_forecasts(trg_test, \n",
    "                                        future_covariates= covs_use,\n",
    "                                        start=ts_test_piped.get_index_at_point(n_lags),\n",
    "                                        verbose=True,\n",
    "                                        stride=n_ahead // 8, \n",
    "                                        forecast_horizon=n_ahead, \n",
    "                                        retrain=False, \n",
    "                                        last_points_only=False, # leave this as False unless you want the output to be one series, the rest will not work with this however\n",
    "                                        #num_samples=30 # only enable when using quantile regression likelihood\n",
    "                                        )\n",
    "    \n",
    "    historics_per_model[model_name] = historics # storing the forecasts in batches of the forecasting horizon, for plot 1\n",
    "    ts_predictions = ts_list_concat(historics) # concatenating the batches into a single time series for plot 2\n",
    "    ts_predictions_inverse = pipeline.inverse_transform(ts_predictions, partial=True) # inverse transform the predictions, we need the original values for the evaluation\n",
    "    ts_predictions_per_model[model_name] = ts_predictions_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smapes_per_model = []\n",
    "df_nrmse_per_model = []\n",
    "\n",
    "for model_name, historics in historics_per_model.items():\n",
    "    df_list = get_df_compares_list(historics, trg_test)\n",
    "    means_per_timestep = pd.concat(df_list, axis=1).filter(like = \"gt\").mean(axis=1).values # mean of the ground truth values on a timestep basis\n",
    "    diffs = get_df_diffs(df_list)\n",
    "    df_smapes = abs(diffs).mean(axis =1) / means_per_timestep # mean of the relative errors on a timestep basis\n",
    "    df_smapes.columns = [model_name]\n",
    "    df_nrmse = np.square(diffs).mean(axis =1) / means_per_timestep # mean of the relative errors on a timestep basis\n",
    "    df_nrmse.columns = [model_name]\n",
    "\n",
    "    df_smapes_per_model.append(df_smapes)\n",
    "    df_nrmse_per_model.append(df_nrmse)\n",
    "\n",
    "df_smapes_per_model = pd.concat(df_smapes_per_model, axis=1)\n",
    "df_smapes_per_model.columns = model_names\n",
    "df_nrmse_per_model = pd.concat(df_nrmse_per_model, axis=1)\n",
    "df_nrmse_per_model.columns = model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smapes_per_model.plot(figsize=(10,10))\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('MAPE [%]')\n",
    "plt.legend(loc = 'lower right', ncol = 2)\n",
    "plt.title(f\"Plot I a): Mean Absolute Percentage Error of the Historical Forecasts in {location}\")\n",
    "\n",
    "df_nrmse_per_model.plot(figsize=(10,10))\n",
    "plt.xlabel('Horizon')\n",
    "plt.ylabel('nRMSE [%]')\n",
    "plt.legend(loc = 'lower right', ncol = 2)\n",
    "plt.title(f\"Plot I b): Normalised RMSE of the Historical Forecasts in {location}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
