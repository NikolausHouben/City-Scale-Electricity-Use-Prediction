{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "from darts.metrics import smape, mape, mase, mse, rmse, r2_score, mae, max_peak_error, mean_n_peak_error\n",
    "\n",
    "from utils import *\n",
    "from train_eval import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaushouben\u001b[0m (\u001b[33mwattcast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\nik\\Desktop\\Berkeley_Projects\\WattCast\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "os.chdir(r\"..\") # should be the git repo root directory, checking below:\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "assert os.getcwd()[-8:] == \"WattCast\"\n",
    "dir_path = os.path.join(os.getcwd(), 'data', 'clean_data')\n",
    "model_dir = os.path.join(os.getcwd(), 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_dict, resolutions_dict = get_hdf_keys(dir_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(scale, location):\n",
    "\n",
    "\n",
    "    units_dict = {'county': 'GW', 'town': 'MW', 'village': 'kW', 'neighborhood': 'kW'}\n",
    "    \n",
    "    \n",
    "    tuned_models = [\n",
    "                    #'rf',\n",
    "                    'lgbm',\n",
    "                    # 'xgb',\n",
    "                    # 'gru',\n",
    "                    # 'nbeats'\n",
    "                    ]\n",
    "\n",
    "    resolution = 60\n",
    "\n",
    "\n",
    "    config_per_model = {}\n",
    "    for model in tuned_models:\n",
    "        config, name = get_best_run_config('Wattcast_tuning', '+eval_loss', model, scale)\n",
    "        config['horizon_in_hours'] = 48\n",
    "        config['location'] = location \n",
    "        config_per_model[model] = config, name\n",
    "\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "    name_id = scale + \"_\" + location + \"_\" + str(resolution) + \"min\"\n",
    "    wandb.init(project=\"Wattcast_2.0\", name=name_id, id = name_id)\n",
    "\n",
    "\n",
    "    config = Config().from_dict(config_per_model[tuned_models[0]][0])\n",
    "\n",
    "    pipeline, ts_train_piped, ts_val_piped, ts_test_piped, ts_train_weather_piped, ts_val_weather_piped, ts_test_weather_piped, trg_train_inversed, trg_val_inversed, trg_test_inversed = data_pipeline(config)\n",
    "\n",
    "    model_instances = get_model_instances(tuned_models, config_per_model)\n",
    "\n",
    "    trained_models, model_instances = load_trained_models(config, model_instances)\n",
    "\n",
    "\n",
    "    if len(model_instances) > 0:\n",
    "        just_trained_models, run_times = train_models(model_instances.values(), \n",
    "                                ts_train_piped,\n",
    "                                ts_train_weather_piped if config.weather else None, \n",
    "                                ts_val_piped,\n",
    "                                ts_val_weather_piped if config.weather else None,\n",
    "                                )\n",
    "        \n",
    "        df_runtimes = pd.DataFrame.from_dict(run_times, orient='index', columns=['runtime']).reset_index()\n",
    "        wandb.log({\"runtimes\": wandb.Table(dataframe=df_runtimes)})\n",
    "        trained_models.extend(just_trained_models)\n",
    "\n",
    "    \n",
    "    models_dict = {model.__class__.__name__: model for model in trained_models}\n",
    "    save_models_to_disk(config, models_dict)\n",
    "\n",
    "    config.model_names = list(models_dict.keys())\n",
    "\n",
    "    config.unit = units_dict[scale.split('_')[1]]\n",
    "\n",
    "    wandb.config.update(config.data)\n",
    "    \n",
    "\n",
    "\n",
    "    return config, models_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(config, models_dict):\n",
    "    \n",
    "    pipeline, ts_train_piped, ts_val_piped, ts_test_piped, ts_train_weather_piped, ts_val_weather_piped, ts_test_weather_piped, trg_train_inversed, trg_val_inversed, trg_test_inversed = data_pipeline(config)\n",
    "    \n",
    "    test_sets = { # see data_prep.ipynb for the split\n",
    "            \"Winter\": (\n",
    "                        ts_val_piped[config.longest_ts_val_idx], \n",
    "                        None if not config.weather else ts_val_weather_piped[config.longest_ts_val_idx],\n",
    "                        trg_val_inversed\n",
    "                        ), \n",
    "            \"Summer\": (\n",
    "                        ts_test_piped[config.longest_ts_test_idx],\n",
    "                        None if not config.weather else ts_test_weather_piped[config.longest_ts_test_idx],\n",
    "                        trg_test_inversed\n",
    "                        )\n",
    "                        }\n",
    "    \n",
    "\n",
    "\n",
    "    dict_result_season = _eval(models_dict, pipeline, test_sets, config)\n",
    "\n",
    "    dict_result_n_ahead = extract_forecasts_per_horizon(config, dict_result_season)\n",
    "\n",
    "    return dict_result_n_ahead\n",
    "\n",
    "def _eval(models_dict, pipeline, test_sets, config):\n",
    "    dict_result_season = {}\n",
    "    for season, (ts, ts_cov, gt) in test_sets.items():\n",
    "        print(f\"Testing on {season} data\")\n",
    "        # Generating Historical Forecasts for each model\n",
    "        ts_predictions_per_model = {}\n",
    "        historics_per_model = {}\n",
    "        for model_name, model in models_dict.items():\n",
    "            print(f\"Generating historical forecasts with {model_name}\")\n",
    "            historics = model.historical_forecasts(ts, \n",
    "                                                future_covariates= ts_cov if model.supports_future_covariates else None,\n",
    "                                                start=ts.get_index_at_point(config.n_lags),\n",
    "                                                verbose=True,\n",
    "                                                stride= 1, # this allows us to later differentiate between the different horizons\n",
    "                                                forecast_horizon= config.timesteps_per_hour*48, # 48 hours is our max horizon\n",
    "                                                retrain=False, \n",
    "                                                last_points_only=False,\n",
    "                                                )\n",
    "            \n",
    "            historics_inverted = [pipeline.inverse_transform(historic) for historic in historics][1:] # the first historic is partly nan, so we skip it\n",
    "            historics_per_model[model_name] = historics_inverted # storing the forecasts in batches of the forecasting horizon, for plot 2\n",
    "        \n",
    "        dict_result_season[season] = historics_per_model, gt\n",
    "\n",
    "    return dict_result_season\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_forecasts_per_horizon(config, dict_result_season):\n",
    "\n",
    "    n_aheads = [i * config.timesteps_per_hour for i in [\n",
    "                                                        1, \n",
    "                                                        4, \n",
    "                                                        8, \n",
    "                                                        24, \n",
    "                                                        48\n",
    "                                                        ]] # horizon in hours\n",
    "    dict_result_n_ahead = {}\n",
    "\n",
    "    for n_ahead in n_aheads:\n",
    "        dict_result_season_update = {}\n",
    "        for season, (historics_per_model, gt) in dict_result_season.items():\n",
    "            ts_predictions_per_model = {}\n",
    "            historics_per_model_update = {}\n",
    "            for model_name, historics in historics_per_model.items():\n",
    "\n",
    "                ts_predictions = ts_list_concat_new(historics, n_ahead)\n",
    "                ts_predictions_per_model[model_name] = ts_predictions\n",
    "                historics_per_model_update[model_name] = historics\n",
    "\n",
    "            ts_predictions_per_model['24-Hour Persistence'] = gt.shift(config.timesteps_per_hour*24) # adding the 24-hour persistence model as a benchmark\n",
    "            dict_result_season_update[season] = historics_per_model_update, ts_predictions_per_model, gt\n",
    "        dict_result_n_ahead[n_ahead] = dict_result_season_update\n",
    "\n",
    "    return dict_result_n_ahead\n",
    "\n",
    "\n",
    "\n",
    "def get_run_results(dict_result_n_ahead, config):\n",
    "    \n",
    "    \n",
    "\n",
    "    df_metrics = error_metrics_table(dict_result_n_ahead, config)\n",
    "\n",
    "    wandb.log({f\"Error metrics\": wandb.Table(dataframe=df_metrics)})\n",
    "\n",
    "    side_by_side(dict_result_n_ahead, config)\n",
    "\n",
    "    error_metric_trajectory(dict_result_n_ahead, config)\n",
    "\n",
    "    error_distribution(dict_result_n_ahead, config)\n",
    "\n",
    "    daily_sum(dict_result_n_ahead, config)\n",
    "\n",
    "\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def error_metrics_table(dict_result_n_ahead, config):\n",
    "\n",
    "    print(\"Calculating error metrics\")\n",
    "    \n",
    "    list_metrics = [rmse, r2_score, mae, smape, mape, max_peak_error , mean_n_peak_error] # evaluation metrics\n",
    "\n",
    "    metrics_tables = []\n",
    "\n",
    "    for n_ahead, dict_result_season in dict_result_n_ahead.items():\n",
    "        for season, (_, preds_per_model, gt) in dict_result_season.items():\n",
    "            df_metrics = get_error_metric_table(list_metrics, preds_per_model, gt)\n",
    "            rmse_persistence = df_metrics.loc[df_metrics.index == '24-Hour Persistence', 'rmse'].values[0]\n",
    "            df_metrics.drop(labels= [config.model_names[-1]], axis = 0, inplace=True)\n",
    "            df_metrics.reset_index(inplace=True)\n",
    "            df_metrics['season'] = season\n",
    "            df_metrics.set_index('season', inplace=True)\n",
    "            df_metrics.reset_index(inplace=True)\n",
    "            df_metrics['horizon_in_hours'] = n_ahead//config.timesteps_per_hour\n",
    "            df_metrics.set_index('horizon_in_hours', inplace=True)\n",
    "            df_metrics.reset_index(inplace=True)\n",
    "            df_metrics['rmse_skill_score'] = 1 - df_metrics['rmse'] / rmse_persistence\n",
    "            metrics_tables.append(df_metrics)\n",
    "\n",
    "    df_metrics = pd.concat(metrics_tables, axis=0, ignore_index=True).sort_values(by=['season', 'horizon_in_hours'])\n",
    "\n",
    "    return df_metrics\n",
    "\n",
    "\n",
    "def side_by_side(dict_result_n_ahead, config):\n",
    "\n",
    "    print(\"Plotting side-by-side comparison of predictions and the ground truth\")\n",
    "    \n",
    "    df_cov_train = pd.read_hdf(os.path.join(dir_path, f'{config.spatial_scale}.h5'), key=f'{config.location}/{config.temp_resolution}min/train_cov')\n",
    "    df_cov_val = pd.read_hdf(os.path.join(dir_path, f'{config.spatial_scale}.h5'), key=f'{config.location}/{config.temp_resolution}min/val_cov')\n",
    "    df_cov_test = pd.read_hdf(os.path.join(dir_path,f'{config.spatial_scale}.h5'), key=f'{config.location}/{config.temp_resolution}min/test_cov')\n",
    "    \n",
    "    \n",
    "    temp_data = {'Summer': df_cov_test.iloc[:,0], 'Winter': df_cov_val.iloc[:,0]}\n",
    "\n",
    "\n",
    "    for n_ahead, dict_result_season in dict_result_n_ahead.items():\n",
    "\n",
    "        for season, (_, preds_per_model, gt) in dict_result_season.items():\n",
    "            fig = go.Figure()\n",
    "\n",
    "            # Add the ground truth data to the left axis\n",
    "            fig.add_trace(go.Scatter(x=gt.pd_series().index, y=gt.pd_series().values, name=\"Ground Truth\", yaxis=\"y1\"))\n",
    "\n",
    "            for model_name in config.model_names:\n",
    "                preds = preds_per_model[model_name]\n",
    "                fig.add_trace(go.Scatter(x=preds.pd_series().index, y=preds.pd_series().values, name=model_name, yaxis=\"y1\"))\n",
    "\n",
    "            # Add the df_cov_test data to the right axis\n",
    "            \n",
    "            series_weather = temp_data[season]\n",
    "            fig.add_trace(go.Scatter(\n",
    "            x=series_weather.index,\n",
    "            y=series_weather.values,\n",
    "            name=\"temperature\",\n",
    "            yaxis=\"y2\",\n",
    "            line=dict(dash=\"dot\", color = 'grey'),  # Set the line style to dotted\n",
    "        ))\n",
    "\n",
    "            fig.update_layout(\n",
    "                title=f\"{season} - Horizon: {n_ahead// config.timesteps_per_hour} Hours\",\n",
    "                xaxis=dict(title=\"Time\"),\n",
    "                yaxis=dict(title=f\"Power [{config.unit}]\", side=\"left\"),\n",
    "                yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    "            )\n",
    "\n",
    "            wandb.log({f\"{season} - Side-by-side comparison of predictions and the ground truth\": fig})\n",
    "\n",
    "\n",
    "\n",
    "def error_metric_trajectory(dict_result_n_ahead, config):\n",
    "\n",
    "    print(\"Plotting error metric trajectory\")\n",
    "\n",
    "    n_ahead, dict_result_season = list(dict_result_n_ahead.items())[-1]\n",
    "\n",
    "    dict_result_season = dict_result_n_ahead[n_ahead]\n",
    "    df_smapes_per_season = {}\n",
    "    df_nrmse_per_season = {}\n",
    "\n",
    "    for season, (historics_per_model, _, gt) in dict_result_season.items():\n",
    "        df_smapes_per_model = []\n",
    "        df_rmse_per_model = []\n",
    "        for model_name, historics in historics_per_model.items():\n",
    "            df_list = get_df_compares_list(historics, gt)\n",
    "            diffs = get_df_diffs(df_list)\n",
    "            df_smapes = abs(diffs).mean(axis =1) \n",
    "            df_smapes.columns = [model_name]\n",
    "            df_rmse = np.square(diffs).mean(axis =1) \n",
    "            df_rmse.columns = [model_name]\n",
    "\n",
    "            df_smapes_per_model.append(df_smapes)\n",
    "            df_rmse_per_model.append(df_rmse)\n",
    "\n",
    "        df_smapes_per_model = pd.concat(df_smapes_per_model, axis=1).ewm(alpha=0.1).mean()\n",
    "        df_smapes_per_model.columns = config.model_names\n",
    "        df_nrmse_per_model = pd.concat(df_rmse_per_model, axis=1).ewm(alpha=0.1).mean()\n",
    "        df_nrmse_per_model.columns = config.model_names\n",
    "        df_smapes_per_season[season] = df_smapes_per_model\n",
    "        df_nrmse_per_season[season] = df_nrmse_per_model\n",
    "\n",
    "    for season in dict_result_season.keys():\n",
    "        fig = df_smapes_per_season[season].plot(figsize=(10,5))\n",
    "        plt.xlabel('Horizon')\n",
    "        plt.ylabel('MAPE [%]')\n",
    "        plt.legend(loc = 'upper left', ncol = 2)\n",
    "        plt.xticks(np.arange(0, n_ahead, 2))\n",
    "        plt.title(f\"Mean Absolute Percentage Error of the Historical Forecasts in {season}\")\n",
    "        wandb.log({f\"MAPE of the Historical Forecasts in {season}\": wandb.Image(fig)})\n",
    "        \n",
    "    for season in dict_result_season.keys():\n",
    "        fig = df_nrmse_per_season[season].plot(figsize=(10,5))\n",
    "        plt.xlabel('Horizon')\n",
    "        plt.ylabel(f'RMSE [{config.unit}]')\n",
    "        plt.xticks(np.arange(0, n_ahead, 2))\n",
    "        plt.legend(loc = 'upper left', ncol = 2)\n",
    "        plt.title(f\"Root Mean Squared Error of the Historical Forecasts in {season}\")\n",
    "        wandb.log({f\"RMSE of the Historical Forecasts in {season}\": wandb.Image(fig)})\n",
    "\n",
    "\n",
    "\n",
    "def error_distribution(dict_result_n_ahead, config):\n",
    "\n",
    "    print(\"Plotting error distribution\")\n",
    "\n",
    "    n_ahead, dict_result_season = list(dict_result_n_ahead.items())[-1]\n",
    "    for season, (historics_per_model, _, gt) in dict_result_season.items():\n",
    "        df_smapes_per_model = []\n",
    "        df_nrmse_per_model = []\n",
    "        fig, ax = plt.subplots(ncols=len(config.model_names), figsize=(5*len(config.model_names),5))\n",
    "        fig.suptitle(f\"Error Distribution of the Historical Forecasts in {season}\")\n",
    "        for i, (model_name, historics) in enumerate(historics_per_model.items()):\n",
    "            df_list = get_df_compares_list(historics, gt)\n",
    "            diffs = get_df_diffs(df_list)\n",
    "            diffs_flat = pd.Series(diffs.values.reshape(-1,))\n",
    "            ax[i].hist(diffs_flat, bins=100)\n",
    "            ax[i].set_title(model_name)\n",
    "        \n",
    "        wandb.log({f\"Error Distribution of the Historical Forecasts in {season}\": wandb.Image(fig)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def daily_sum(dict_result_n_ahead, config):\n",
    "\n",
    "    print(\"Plotting daily sum of the predictions and the ground truth\")\n",
    "\n",
    "    dict_result_season = dict_result_n_ahead[list(dict_result_n_ahead.keys())[-1]]\n",
    "    for season, (_, preds_per_model, gt) in dict_result_season.items():\n",
    "        dfs_daily_sums = []\n",
    "        for model_name, preds in preds_per_model.items():\n",
    "            df_preds = preds.pd_series().to_frame(model_name + \"_preds\")\n",
    "            z = df_preds.groupby(df_preds.index.date).sum()\n",
    "            dfs_daily_sums.append(z)\n",
    "\n",
    "        df_gt = gt.pd_series().to_frame(\"ground_truth\") \n",
    "        z = df_gt.groupby(df_gt.index.date).sum() / config.timesteps_per_hour\n",
    "        dfs_daily_sums.append(z)\n",
    "        df_compare = pd.concat(dfs_daily_sums, axis=1).dropna()\n",
    "        fig = df_compare[:10].plot(kind='bar', figsize=(20,10))\n",
    "        plt.legend(loc = 'upper right', ncol = 2)\n",
    "        plt.ylabel(f'Energy [{config.unit}h]')\n",
    "        plt.title(f\"Daily Sum of the Predictions and the Ground Truth in {season}\")\n",
    "        wandb.log({f\"Daily Sum of the Predictions and the Ground Truth in {season}\": wandb.Image(fig)})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO log if used weather and boxcox for ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Los_Angeles at 1_county scale\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by +summary_metrics.eval_loss\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047023469d0846bea843fb0b162f5f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\nik\\AppData\\Local\\Temp\\ipykernel_5536\\102373165.py 30 training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 1144, in init\n",
      "    run = wi.init()\n",
      "  File \"c:\\Users\\nik\\miniconda3\\envs\\gpu2\\lib\\site-packages\\wandb\\sdk\\wandb_init.py\", line 773, in init\n",
      "    raise error\n",
      "wandb.errors.CommError: Error communicating with wandb process, exiting...\n",
      "For more info see: https://docs.wandb.ai/guides/track/tracking-faq#initstarterror-error-communicating-with-wandb-process-\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for scale, locations in locations_dict.items():\n",
    "\n",
    "    scale = scale.split('.')[0]\n",
    "    for location in locations:\n",
    "\n",
    "        print(f\"Training {location} at {scale} scale\")\n",
    "\n",
    "        config, models = training(scale, location)\n",
    "\n",
    "        eval_dict = evaluation(config, models)\n",
    "\n",
    "        df_metrics = get_run_results(eval_dict, config)\n",
    "\n",
    "        #wandb.finish()\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"Error metrics\": wandb.Table(dataframe=df_metrics)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
