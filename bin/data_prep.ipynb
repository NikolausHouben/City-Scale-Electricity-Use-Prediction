{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os   \n",
    "import h5py\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from utils import (remove_days, standardize_format, \n",
    "                   remove_non_positive_values, split_train_val_test_datasets, \n",
    "                   get_weather_data, calculate_stats_and_plot_hist, remove_outliers)\n",
    "\n",
    "from timezonefinder import TimezoneFinder\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set working directory\n",
    "os.chdir(r\"..\") # should be the git repo root directory\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "assert os.getcwd()[-8:] == \"WattCast\", \"Current working directory is not the git repo root directory\"\n",
    "\n",
    "raw_data_path = os.path.join(os.getcwd(),'data','raw_data')\n",
    "if not os.path.exists(raw_data_path):\n",
    "    os.makedirs(raw_data_path)\n",
    "    print(\"Created directory: \" + raw_data_path)\n",
    "\n",
    "save_path = os.path.join(os.getcwd(),'data','clean_data')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    print(\"Created directory: \" + save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Raw data import, visualization & cleaning<u/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Structure of Blocks:\n",
    "\n",
    "1. Each block starts with the Name of the dataset and its download link, alternatively you can follow this [link](https://www.dropbox.com/sh/fvx3wune2qg2x43/AADP4F3UwIqrS9tYnN6mTob5a?dl=0) to download the raw and cleaned data directory from our gdrive.\n",
    "2. The overview states for which aggregation level the data was used. For example, while (1) BA dataset is only used on the county level, the (2) Substation dataset from Germany is used for both the neighborhood level in an aggregated form and on the household level in its disaggregated form.\n",
    "3. Power and Weather data are then imported and cleaned, wherever necessary.\n",
    "4. Each block also provides an interactive visualization of the timeseries, often resampled to a lower temporal resolution to make it run faster.\n",
    "5. The block ends with spliting* the data into one train (1 year) and two test datasets (one in summer and winter); once more 60 minute and, if available, in 15 minute resolution\n",
    "\n",
    "a) <u>Note</u> on the train test splits: Not all data was recorded in the same temporal resolution and during the same time. However, training and testing set lengths were kept consistent to ensure a fair comparison of algorithms across scales. \n",
    "\n",
    "b) <u>Note</u> on the timezones: Data comes in various timezones and are always processed to match the local timezone. This helps with interepretability of results due to known patters of human behaviour."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2460; 'Portland Feeder'\n",
    "\n",
    "\n",
    "Used for: \n",
    "* 2_town\n",
    "----------\n",
    "* Duration: 1 year\n",
    "* Resolution: 60 minutes\n",
    "* Power Level: 1-50 MW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this path to the directory where the data is stored\n",
    "\n",
    "dir_path = os.path.join(raw_data_path,'Portland_feeder_ts.csv')\n",
    "\n",
    "dir_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_test_split_month(df, train_months=None, val_months = None, test_months = None):\n",
    "\n",
    "    '''\n",
    "    Splits a dataframe into train, validation and test sets based on the month of the year.\n",
    "    The months are first ranked based on the maximum value in that month.\n",
    "    Train should contain the months with the highest and lowest maximum values.\n",
    "    Validation should contain the month with the second highest maximum value.\n",
    "    Test should contain the month with the second lowest maximum value.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if train_months is None:\n",
    "        months_ranked = df.groupby(df.index.month).max().sort_values(by = df.columns[0], ascending = False).index\n",
    "        train_months = [months_ranked[0]] + [months_ranked[-1]] + months_ranked[2:-2].tolist()\n",
    "        val_months = [months_ranked[-2]]\n",
    "        test_months = [months_ranked[1]]\n",
    "\n",
    "    df_train = df[df.index.month.isin(train_months)]\n",
    "    df_val = df[df.index.month.isin(val_months)]\n",
    "    df_test = df[df.index.month.isin(test_months)]\n",
    "\n",
    "    return df_train, df_val, df_test, train_months, val_months, test_months\n",
    "\n",
    "\n",
    "\n",
    "def train_val_test_split_visual(df, train_months = None, val_months = None, test_months = None):\n",
    "\n",
    "    '''\n",
    "    Splits a dataframe into train, validation and test sets based on the month of the year.\n",
    "    Determined visually\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if train_months is None:\n",
    "        train_months = [1,3,4,5,7,8,9,10,11,12]\n",
    "        val_months = [2]\n",
    "        test_months = [6]\n",
    "\n",
    "    df_train = df[df.index.month.isin(train_months)]\n",
    "    df_val = df[df.index.month.isin(val_months)]\n",
    "    df_test = df[df.index.month.isin(test_months)]\n",
    "\n",
    "    return df_train, df_val, df_test, train_months, val_months, test_months\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ami = pd.read_csv(dir_path, parse_dates=True, index_col=0)\n",
    "\n",
    "df_ami.columns = [col.replace(\" \", \".\")for col in df_ami.columns]\n",
    "\n",
    "scales = list(set([col.split(\"-\")[0] for col in df_ami.columns.tolist()[:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ami.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "location_acronnyms = {col:col.split(\"-\")[1] for col in df_ami.columns[:-1]}\n",
    "coords = (45.514904014844944, -122.65894786115099)\n",
    "\n",
    "unit = \"MW\" # unit of the demand data\n",
    "temp_resolutions = [60] # minutes, this dataset is only available in hourly resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_acronnyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat, lng = coords\n",
    "temp_resolution = temp_resolutions[0]\n",
    "tf = TimezoneFinder()\n",
    "tz = tf.timezone_at(lng=lng, lat=lat)\n",
    "for spatial_scale in scales:\n",
    "    store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "    dfs = {}\n",
    "    df_scale = df_ami.filter(like=spatial_scale)\n",
    "    for col in df_scale.columns:\n",
    "        print(f\"Preparing data for {col}...\")\n",
    "        # power\n",
    "        value = location_acronnyms[col]\n",
    "        df = df_scale[[col]]\n",
    "        print(df.columns)\n",
    "        # df = df.tz_localize('UTC').tz_convert(tz).tz_localize(None), data is already local\n",
    "        df = standardize_format(df, 'load', temp_resolution, col, unit)\n",
    "        df = remove_outliers(df, df.columns[0], lower_percentile=1, upper_percentile=99.9)\n",
    "        df = remove_non_positive_values(df, set_nan=True)\n",
    "        train, val, test, train_months, val_months, test_months = train_val_test_split_visual(df)\n",
    "        dfs[value] = df\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        # weather\n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather, _, _, _ = train_val_test_split_visual(df_weather, train_months, val_months, test_months)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "\n",
    "    store.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_hdf_keys\n",
    "\n",
    "clean_data_path = os.path.join(os.getcwd(), \"data\", \"clean_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hdf_keys(clean_data_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_location_splits(clean_data_path,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = '13596.(MWh)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we allow the model to use the data from the same scale, if it is available\n",
    "auxilary_data = []\n",
    "if config.use_data_from_same_scale:\n",
    "    data = load_data(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningBerkeley",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bd490ff79fd304cef95a146bac43660f32d9e1d5f3555df986ea8b6053493f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
