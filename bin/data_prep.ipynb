{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os   \n",
    "import h5py\n",
    "from tslearn.metrics import dtw\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sys.path.append('../')\n",
    "from utils.data_utils import *\n",
    "from utils.paths import CLEAN_DATA_DIR\n",
    "\n",
    "from timezonefinder import TimezoneFinder\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/nikolaushouben/Desktop/WattCast\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set working directory\n",
    "os.chdir(r\"..\") # should be the git repo root directory\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "assert os.getcwd()[-8:] == \"WattCast\", \"Current working directory is not the git repo root directory\"\n",
    "\n",
    "raw_data_path = os.path.join(os.getcwd(),'data','raw_data')\n",
    "if not os.path.exists(raw_data_path):\n",
    "    os.makedirs(raw_data_path)\n",
    "    print(\"Created directory: \" + raw_data_path)\n",
    "\n",
    "save_path = os.path.join(os.getcwd(),'data','clean_data')\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    print(\"Created directory: \" + save_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Raw data import, visualization & cleaning<u/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria for Data Collection: \n",
    "\n",
    "* at least 1 year of training data and 1 summer and 1 winter month of testing data are available\n",
    "* at least 1 hour resolution (higher resolution preferred)\n",
    "* less than 1% missing values or invalid measurements"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Structure of Blocks:\n",
    "\n",
    "1. Each block starts with the Name of the dataset and its download link, alternatively you can follow this [link](https://www.dropbox.com/sh/fvx3wune2qg2x43/AADP4F3UwIqrS9tYnN6mTob5a?dl=0) to download the raw and cleaned data directory from our gdrive.\n",
    "2. The overview states for which aggregation level the data was used. For example, while (1) BA dataset is only used on the county level, the (2) Substation dataset from Germany is used for both the neighborhood level in an aggregated form and on the household level in its disaggregated form.\n",
    "3. Power and Weather data are then imported and cleaned, wherever necessary.\n",
    "4. Each block also provides an interactive visualization of the timeseries, often resampled to a lower temporal resolution to make it run faster.\n",
    "5. The block ends with spliting* the data into one train (1 year) and two test datasets (one in summer and winter); once more 60 minute and, if available, in 15 minute resolution\n",
    "\n",
    "a) <u>Note</u> on the train test splits: Not all data was recorded in the same temporal resolution and during the same time. However, training and testing set lengths were kept consistent to ensure a fair comparison of algorithms across scales. \n",
    "\n",
    "b) <u>Note</u> on the timezones: Data comes in various timezones and are always processed to match the local timezone. This helps with interepretability of results due to known patters of human behaviour."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2460; 'EIA Cleaned Hourly Electricity Demand Data' - Balancing Authority (United States of America)\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3690240.svg)](https://doi.org/10.5281/zenodo.3690240)\n",
    "\n",
    "Used for: \n",
    "* 1_county\n",
    "----------\n",
    "* Duration: 3 years\n",
    "* Resolution: 60 minutes\n",
    "* Power Level: 1-50 GW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in https://doi.org/10.1016/j.adapen.2021.100025, We used the data from three BAs: the Los Angeles Department of Water and Power (LADWP), Balance Authority of Northern California (BANC), and New York Independent System Operator (NYISO),"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set this path to the directory where the data is stored\n",
    "\n",
    "dir_path = os.path.join(raw_data_path,'balancing_authorities_US','power','{}.csv')\n",
    "\n",
    "\n",
    "dir_path_weather = os.path.join(raw_data_path,'balancing_authorities_US','weather','{}.csv')\n",
    "county_acronnyms = {\"LDWP\":\"Los_Angeles\", \"BANC\":\"Sacramento\", \"NYIS\":\"New_York\"}\n",
    "\n",
    "county_coordinates = {\"Los_Angeles\":(34.08851, -118.234216), \"Sacramento\":(38.5815719, -121.4943996), \"New_York\":(40.730610, -73.935242)}\n",
    "\n",
    "unit = \"GW\" # unit of the demand data\n",
    "temp_resolutions = [60] # minutes, this dataset is only available in hourly resolution\n",
    "spatial_scale = \"1_county\" # spatial scale of the data\n",
    "\n",
    "# Determined visually to meet criteria mentioned above\n",
    "train_begin = \"2016-01-01\"\n",
    "train_end = \"2017-01-01\"\n",
    "# Winter\n",
    "val_begin = \"2017-01-01\"\n",
    "val_end = \"2017-02-01\"\n",
    "#Summer\n",
    "test_begin = \"2017-08-15\" # select a period with a heat wave\n",
    "test_end = \"2017-09-15\"  # select a period with a heat wave\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "for temp_resolution in temp_resolutions:\n",
    "    dfs = {}\n",
    "    for key, value in county_acronnyms.items():\n",
    "        \n",
    "        lat, lng = county_coordinates[value]\n",
    "        tf = TimezoneFinder()\n",
    "        tz = tf.timezone_at(lng=lng, lat=lat)\n",
    "        # power\n",
    "        df = pd.read_csv(dir_path.format(key), index_col=0,parse_dates=True)\n",
    "        df = df.loc[df[\"category\"] == \"OKAY\"]['cleaned demand (MW)'].to_frame(f'{value}_demand_{unit}')/1e3 # only keep the demand data that is labeled as \"OKAY\" see paper for more details\n",
    "        df = df.tz_localize('UTC').tz_convert(tz).tz_localize(None)\n",
    "        df = standardize_format(df, 'load', temp_resolution, key, unit)\n",
    "        df = remove_non_positive_values(df, set_nan=True)\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        # weather\n",
    "\n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "\n",
    "        \n",
    "store.close()\n",
    "# Merge all the dataframes into one\n",
    "df_county = pd.concat(dfs.values(), axis=1)\n",
    "df_county.columns = dfs.keys()\n",
    "df_county.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "location = 'Sacramento'\n",
    "df = df_county\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add the ground truth data to the left axis\n",
    "fig.add_trace(go.Scatter(y=df[location], x=df.index, name = location,  yaxis=\"y1\"))\n",
    "fig.add_trace(go.Scatter(y=df[location+'weather'], x=df.index, name = 'Temp',  yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time\"),\n",
    "    yaxis=dict(title=f\"Power\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_stats_and_plot_hist(df_county[[col for col in df_county.columns if not 'weather' in col]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2461; 'Electricity Load Diagrams' - Substation data (Portugal)\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3690240.svg)](https://doi.org/10.24432/C58C86)\n",
    "\n",
    "Used for: \n",
    "\n",
    "* 2_town\n",
    "* 3_village\n",
    "\n",
    "----------\n",
    "* Duration: 4 years\n",
    "* Resolution: 15 minutes\n",
    "* Power Level: 0.05-50 MW (this broad range allows using this dataset for the two spatial scales: town & village)\n",
    "* Timestamp: All time labels of the power data report to Portuguese hour (see https://archive-beta.ics.uci.edu/dataset/321/electricityloaddiagrams20112014)\n",
    "\n",
    "\n",
    "Weather data is accessed through the https://open-meteo.com/en/docs API."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join(raw_data_path,'substation_PT','LD2011_2014.txt')\n",
    "temp_resolutions = [15, 60] # minutes\n",
    "\n",
    "\n",
    "# Determined visually to meet criteria mentioned above\n",
    "train_begin = \"2011-01-01\"\n",
    "train_end = \"2012-01-01\"\n",
    "# Winter\n",
    "val_begin = \"2012-01-01\"\n",
    "val_end = \"2012-02-01\"\n",
    "#Summer\n",
    "test_begin = \"2012-06-15\" # select a period with a heat wave\n",
    "test_end = \"2012-07-15\"\n",
    "\n",
    "\n",
    "#data import\n",
    "df_sub = pd.read_csv(dir_path, sep=';', decimal = \",\", index_col=0,parse_dates=True) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2_town "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2_town\n",
    "spatial_scale = \"2_town\" # spatial scale of the data\n",
    "lat = 39.399872 # TODO: get the lat and lng for the town\n",
    "lng = -8.224454\n",
    "unit = \"MW\" # unit of the demand data\n",
    "\n",
    "#select the three columns with the highest median demand\n",
    "df_sub_town = df_sub.loc[:, df_sub.median().sort_values(ascending=False).index].iloc[:, 1:4] / 1e3  # This data is in kW, we want to look at it in MW # the first location is missing the data for 2011 -> 1:4\n",
    "town_acronnyms = {col: f'town_{i}' for i, col in enumerate(df_sub_town.columns)}\n",
    "\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "for temp_resolution in temp_resolutions:\n",
    "# save the data for the towns and villages\n",
    "    dfs = {}\n",
    "    for key, value in town_acronnyms.items():\n",
    "\n",
    "        # power\n",
    "        df = df_sub_town[key].to_frame(f'{value}_demand_{unit}')\n",
    "        df = standardize_format(df, 'load', temp_resolution, value, unit)\n",
    "        df = remove_non_positive_values(df, set_nan=True)\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "\n",
    "store.close()      \n",
    "\n",
    "# Merge all the dataframes into one\n",
    "df_town = pd.concat(dfs.values(), axis=1)\n",
    "df_town.columns = dfs.keys()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'town_0'\n",
    "\n",
    "df= df_town\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add the ground truth data to the left axis\n",
    "fig.add_trace(go.Scatter(y=df[location], x=df.index, name = location,  yaxis=\"y1\"))\n",
    "fig.add_trace(go.Scatter(y=df[location+'weather'], x=df.index, name = 'Temp',  yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time\"),\n",
    "    yaxis=dict(title=f\"Power\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_stats_and_plot_hist(df_town[[col for col in df_town.columns if not 'weather' in col]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3_village "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_village\n",
    "spatial_scale = \"3_village\" # spatial scale of the data\n",
    "lat = 39.399872 # TODO: get the lat and lng for the town\n",
    "lng = -8.224454\n",
    "unit_village = \"kW\" # unit of the demand data\n",
    "\n",
    "middle = df_sub.shape[1] // 2\n",
    "df_sub_vill = df_sub.loc[:, df_sub.median().sort_values(ascending=False).index].iloc[:, middle:middle+5]\n",
    "selected_cols =[\"MT_230\", \"MT_250\", \"MT_278\"]\n",
    "df_sub_vill = df_sub_vill[selected_cols] * 1e3 # convert to kW\n",
    "village_acronnyms = {col: f'village_{i}' for i, col in enumerate(df_sub_vill.columns)}\n",
    "\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w') \n",
    "for temp_resolution in temp_resolutions:\n",
    "    dfs = {}\n",
    "    for key, value in village_acronnyms.items():\n",
    "        df = df_sub_vill[key].to_frame(f'{value}_demand_{unit_village}')\n",
    "        df = standardize_format(df, 'load', temp_resolution, value, unit_village)\n",
    "        df = remove_non_positive_values(df, set_nan=True)\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "\n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "        \n",
    "\n",
    "store.close() \n",
    "# Merge all the dataframes into one\n",
    "df_village = pd.concat(dfs.values(), axis=1)\n",
    "df_village.columns = dfs.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'village_0'\n",
    "\n",
    "df= df_village\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add the ground truth data to the left axis\n",
    "fig.add_trace(go.Scatter(y=df[location], x=df.index, name = location,  yaxis=\"y1\"))\n",
    "fig.add_trace(go.Scatter(y=df[location+'weather'], x=df.index, name = 'Temp',  yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time\"),\n",
    "    yaxis=dict(title=f\"Power {unit_village}\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_stats_and_plot_hist(df_village[[col for col in df_village.columns if not 'weather' in col]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2462; Building Data Genome 2 (BDG2) Data-Set\n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3887306.svg)](https://doi.org/10.5281/zenodo.3887306)\n",
    "\n",
    "\n",
    "Note that the meter data has been cleaned found in the 'building-data-genome-project-2-v1.0\\data\\meters\\cleaned\\' folder. Furthermore, measurements are in kW unit, and the timezone is set to the local timezone. Also note that builds with the usage of 'Education' have been used because they exhibit the most complete data record.\n",
    "\n",
    "Used for: \n",
    "\n",
    "* 4_neighborhood\n",
    "* 5_building\n",
    "\n",
    "----------\n",
    "* Duration: 2 years\n",
    "* Resolution: 60 minutes\n",
    "* Power Level: 0-3000 W"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gen = pd.read_csv(r'data\\raw_data\\building-data-genome-project-2-v1.0\\data\\meters\\cleaned\\electricity_cleaned.csv', index_col=0, parse_dates=True)\n",
    "df_meta = pd.read_csv(r'data\\raw_data\\building-data-genome-project-2-v1.0\\data\\metadata\\metadata.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4_neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.groupby('site_id')['primaryspaceusage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.groupby('site_id')['primaryspaceusage'].value_counts().to_frame().unstack().plot(kind='bar', figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_neighborhood = df_meta.loc[\n",
    "                                        (df_meta['site_id'] == 'Bull') \n",
    "                                         ]\n",
    "\n",
    "df_meta_neighborhood.set_index('building_id', inplace=True)\n",
    "cols_neighborhood = list(set(df_meta_neighborhood.index.to_list()) & set(df_gen.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_neigh = df_gen[cols_neighborhood]\n",
    "px.line(df_plot_neigh.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neighborhoods_1 = pd.DataFrame()\n",
    "\n",
    "neighborhoods = ['Hog', 'Bobcat', 'Bull']\n",
    "for neighborhood in neighborhoods:\n",
    "    df_meta_neighborhood = df_meta.loc[\n",
    "                                        (df_meta['site_id'] == neighborhood) \n",
    "                                         ]\n",
    "    df_meta_neighborhood.set_index('building_id', inplace=True)\n",
    "    cols_neighborhood = list(set(df_meta_neighborhood.index.to_list()) & set(df_gen.columns.to_list()))\n",
    "    df_plot_neigh = df_gen[cols_neighborhood].sum(axis=1).to_frame(f'{neighborhood}')\n",
    "    df_neighborhoods_1 = pd.concat([df_neighborhoods_1, df_plot_neigh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(df, column, lower_percentile=0, upper_percentile=100):\n",
    "    lower_threshold = df[column].quantile(lower_percentile / 100)\n",
    "    upper_threshold = df[column].quantile(upper_percentile / 100)\n",
    "    df_filtered = df[(df[column] >= lower_threshold) & (df[column] <= upper_threshold)]\n",
    "    return df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_resolutions = [60] # minutes\n",
    "# Determined visually to meet criteria mentioned above\n",
    "train_begin = \"2016-01-01\"\n",
    "train_end = \"2017-01-01\"\n",
    "val_begin = \"2017-08-01\"\n",
    "val_end = \"2017-09-01\"\n",
    "test_begin = \"2017-12-01\"\n",
    "test_end = \"2017-12-31\"\n",
    "\n",
    "unit = \"kW\" # unit of the demand data\n",
    "spatial_scale = \"4_neighborhood\" # spatial scale of the data\n",
    "arconyms_neighborhood = {col: f'neighborhood_{i}' for i, col in enumerate(neighborhoods)}\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "for temp_resolution in temp_resolutions:\n",
    "    dfs = {}\n",
    "    for key, value in arconyms_neighborhood.items():\n",
    "\n",
    "        tf = TimezoneFinder()\n",
    "        lat = df_meta.groupby('site_id')['lat'].mean().values[0]\n",
    "        lng = df_meta.groupby('site_id')['lng'].mean().values[0]\n",
    "        tz = tf.timezone_at(lng=lng, lat=lat)\n",
    "        # power\n",
    "        df = df_neighborhoods_1[key].to_frame(f'{value}_demand_{unit}')\n",
    "        #df = df.tz_localize('UTC').tz_convert(tz).tz_localize(None) # the data is already in the correct timezone\n",
    "        df = standardize_format(df, \"load\", temp_resolution, value, unit)\n",
    "        #df = df.apply(lambda x: x.sub(x.groupby(x.index.date).transform(\"min\"))) # to avoid hovering above 0 for some profiles\n",
    "        df = remove_days(df, 0.1)\n",
    "        df = remove_non_positive_values(df, set_nan=True)\n",
    "        df = remove_outliers(df, df.columns[0], lower_percentile=0.05, upper_percentile=99.9)\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        # weather\n",
    "        \n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "        \n",
    "\n",
    "store.close()\n",
    "\n",
    "# Merge all the dataframes into one for visualization\n",
    "df_neighborhoods = pd.concat(dfs.values(), axis=1)\n",
    "df_neighborhoods.columns = dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'neighborhood_0'\n",
    "\n",
    "df= df_neighborhoods\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add the ground truth data to the left axis\n",
    "fig.add_trace(go.Scatter(y=df[location], x=df.index, name = location,  yaxis=\"y1\"))\n",
    "fig.add_trace(go.Scatter(y=df[location+'weather'], x=df.index, name = 'Temp',  yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time\"),\n",
    "    yaxis=dict(title=f\"Power {unit_village}\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_stats_and_plot_hist(df_neighborhoods[[col for col in df_neighborhoods.columns if not 'weather' in col]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta_building = df_meta.loc[\n",
    "                                        (df_meta['primaryspaceusage'] == 'Education') \n",
    "                                         ]\n",
    "\n",
    "df_meta_building.set_index('building_id', inplace=True)\n",
    "cols_building = list(set(df_meta_building.index.to_list()) & set(df_gen.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot_gen = df_gen[cols_building]\n",
    "# limits for max load of each building (we want rather small buildings)\n",
    "max = 12\n",
    "min = 0\n",
    "df_plot_gen = df_plot_gen.loc[:, (df_plot_gen.max() < max) & (df_plot_gen.min() > min)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df_plot_gen, x=df_plot_gen.index, y=df_plot_gen.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following locations have been determined visually to meet the criteria mentioned above\n",
    "buildings = ['Bear_education_Sandy', 'Bear_education_Millie', 'Cockatoo_education_Joel']\n",
    "df_buildings = df_gen[buildings] * 1e3 # convert to W\n",
    "df_meta_buildings = df_meta_building.loc[buildings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_resolutions = [60] # minutes\n",
    "# Determined visually to meet criteria mentioned above\n",
    "train_begin = \"2016-01-01\"\n",
    "train_end = \"2017-01-01\"\n",
    "val_begin = \"2017-08-01\"\n",
    "val_end = \"2017-09-01\"\n",
    "test_begin = \"2017-12-01\"\n",
    "test_end = \"2017-12-31\"\n",
    "\n",
    "unit = \"W\" # unit of the demand data\n",
    "spatial_scale = \"5_building\" # spatial scale of the data\n",
    "arconyms_building = {col: f'building_{i}' for i, col in enumerate(buildings)}\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "for temp_resolution in temp_resolutions:\n",
    "    dfs = {}\n",
    "    for key, value in arconyms_building.items():\n",
    "\n",
    "        tf = TimezoneFinder()\n",
    "        lat, lng = df_meta_buildings.loc[key, 'lat'], df_meta_buildings.loc[key, 'lng']\n",
    "        tz = tf.timezone_at(lng=lng, lat=lat)\n",
    "        # power\n",
    "        df = df_buildings[key].to_frame(f'{value}_demand_{unit}')\n",
    "        #df = df.tz_localize('UTC').tz_convert(tz).tz_localize(None)\n",
    "        df = standardize_format(df, \"load\", temp_resolution, value, unit)\n",
    "        df = df.apply(lambda x: x.sub(x.groupby(x.index.date).transform(\"min\"))) # to avoid hovering above 0 for some profiles\n",
    "        df = remove_days(df, 0.1)\n",
    "        df = remove_non_positive_values(df, set_nan=True)\n",
    "        df = remove_outliers(df, df.columns[0], lower_percentile=0.01, upper_percentile=100)\n",
    "\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        # weather\n",
    "        \n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "        \n",
    "\n",
    "store.close()\n",
    "\n",
    "# Merge all the dataframes into one for visualization\n",
    "df_buildings = pd.concat(dfs.values(), axis=1)\n",
    "df_buildings.columns = dfs.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = 'building_1'\n",
    "\n",
    "df= df_buildings\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add the ground truth data to the left axis\n",
    "fig.add_trace(go.Scatter(y=df[location], x=df.index, name = location,  yaxis=\"y1\"))\n",
    "fig.add_trace(go.Scatter(y=df[location+'weather'], x=df.index, name = 'Temp',  yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time\"),\n",
    "    yaxis=dict(title=f\"Power {unit_village}\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_stats_and_plot_hist(df_buildings[[col for col in df_buildings.columns if not 'weather' in col]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking what is in the saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils import get_hdf_keys\n",
    "# See what keys are in the h5py data file\n",
    "locations_dict, resolutions_dict = get_hdf_keys(dir_path)\n",
    "\n",
    "print(\"Locations: \", locations_dict)\n",
    "print(\"Resolutions: \", resolutions_dict)\n",
    "\n",
    "for scale, locations in locations_dict.items():\n",
    "    if scale == '5_building.h5':\n",
    "        for location in locations:\n",
    "            df_train = pd.read_hdf(os.path.join(dir_path, scale), key=f'{location}/60min/train_target')\n",
    "            df_val = pd.read_hdf(os.path.join(dir_path, scale), key=f'{location}/60min/val_target')\n",
    "            df_test = pd.read_hdf(os.path.join(dir_path, scale), key=f'{location}/60min/test_target')\n",
    "            fig = px.line(df_train, title=f'{scale}: {location}')\n",
    "            fig.add_trace(px.line(df_val, title='Validation Set').data[0])\n",
    "            fig.add_trace(px.line(df_test, title='Test Set').data[0])\n",
    "            fig.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets below are not used in the paper, but might be useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2463; 'WPuQ' - Household data (Germany)\n",
    "Note that since this data was recorded in a village in Germany, the aggregate of the data will be used as a neighborhood demand \n",
    "\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5642902.svg)](https://doi.org/10.5281/zenodo.5642902)\n",
    "\n",
    "Used for: \n",
    "\n",
    "* 4_neighborhood\n",
    "* 5_household\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h5py extraction; only for the german data; cleaning code starts in the next cell\n",
    "resolution = 15\n",
    "dfs_load = []\n",
    "dfs_temperature = []\n",
    "for year in [2018, 2019, 2020]:\n",
    "    # Load\n",
    "    filename = os.path.join(\"data\",\"raw_data\",\"households_GER\",\"power\",f\"{year}_data_{resolution}min.hdf5\")\n",
    "    f = h5py.File(filename)\n",
    "    group_no_pv = f[\"NO_PV\"] #Only regard those profiles that are not mixed with PV generation\n",
    "    dfs = {}\n",
    "\n",
    "    for key in group_no_pv.keys():\n",
    "        table = group_no_pv[key][\"HOUSEHOLD\"]\n",
    "        table = table[\"table\"][:]\n",
    "        df = pd.DataFrame(table).dropna().set_index(\"index\")[[\"P_TOT\"]]\n",
    "        df.index = pd.to_datetime(df.index, unit = \"s\")\n",
    "        dfs[key] = df\n",
    "\n",
    "    df_load = pd.concat(list(dfs.values()), axis=1)\n",
    "    df_load.columns = list(dfs.keys())\n",
    "    dfs_load.append(df_load)\n",
    "\n",
    "    # Weather is available but includes a lot of missing values, so we will use the API instead (next cell) \n",
    "    filename_weather = os.path.join(\"data\",\"raw_data\",\"households_GER\",\"weather\",f\"{year}_weather.hdf5\")\n",
    "\n",
    "    f_weather = h5py.File(filename_weather)\n",
    "    df_temp = pd.DataFrame(f_weather['WEATHER_SERVICE']['IN']['WEATHER_TEMPERATURE_TOTAL']['table'][:]).set_index(\"index\").dropna()\n",
    "    df_temp.index = pd.to_datetime(df_temp.index, unit = \"ns\")\n",
    "    dfs_temperature.append(df_temp)\n",
    "    \n",
    "\n",
    "df_load_final = pd.concat(dfs_load, axis = 0)\n",
    "df_temp_final = pd.concat(dfs_temperature, axis = 0)\n",
    "# subtract the minimum value of each day because some profiles are 'levitating' above 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning code starts here\n",
    "\n",
    "temp_resolutions = [15, 60] # minutes\n",
    "# Determined visually to meet criteria mentioned above\n",
    "train_begin = \"2018-08-01\"\n",
    "train_end = \"2019-08-01\"\n",
    "val_begin = \"2020-01-01\"\n",
    "val_end = \"2020-02-01\"\n",
    "test_begin = \"2019-08-01\"\n",
    "test_end = \"2019-09-01\"\n",
    "\n",
    "\n",
    "\n",
    "# Determined visually to meet criteria mentioned above\n",
    "columns_neighborhood = ['SFH3', 'SFH4', 'SFH5', 'SFH9', 'SFH10', 'SFH12', 'SFH16','SFH18','SFH19', 'SFH21',\n",
    "                        'SFH22', 'SFH23', 'SFH27', 'SFH28', 'SFH29', 'SFH30', 'SFH31',\n",
    "                        'SFH32', 'SFH36', 'SFH38']\n",
    "\n",
    "# for later use in the notebook\n",
    "df_neighborhood_1 = df_load_final[columns_neighborhood].sum(axis=1).to_frame(\"demand\") / 1e3 # convert to kW\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5_household\n",
    "unit = \"W\" # unit of the demand data\n",
    "spatial_scale = \"5_household\" # spatial scale of the data\n",
    "lat = 52.266666\n",
    "lng = 10.516667\n",
    "columns_household = [\"SFH4\", \"SFH36\", \"SFH12\"] # selected columns through visual inspection (not performed here)\n",
    "arconyms_household = {col: f'household_{i}' for i, col in enumerate(columns_household)}\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "for temp_resolution in temp_resolutions:\n",
    "    dfs = {}\n",
    "    for key, value in arconyms_household.items():\n",
    "\n",
    "        tf = TimezoneFinder()\n",
    "        tz = tf.timezone_at(lng=lng, lat=lat)\n",
    "        # power\n",
    "        df = df_load_final[key].to_frame(f'{value}_demand_{unit}')\n",
    "        df = df.tz_localize('UTC').tz_convert(tz).tz_localize(None)\n",
    "        df = standardize_format(df, \"load\", temp_resolution, value, unit)\n",
    "        df = df.apply(lambda x: x.sub(x.groupby(x.index.date).transform(\"min\"))) # to avoid hovering above 0 for some profiles\n",
    "        df = remove_days(df, 0.1)\n",
    "        df = remove_non_positive_values(df)\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        # weather\n",
    "        \n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "        \n",
    "\n",
    "store.close()\n",
    "\n",
    "# Merge all the dataframes into one for visualization\n",
    "df_households = pd.concat(dfs.values(), axis=1)\n",
    "\n",
    "# save the weather data for the neighborhood saving (see below)\n",
    "df_neighborhood_1_weather = df_households.filter(like='temp').iloc[:, 0].to_frame(\"temperature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_households.columns = dfs.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_households.resample(\"4H\").mean()\n",
    "\n",
    "location = 'household_0'\n",
    "\n",
    "fig = go.Figure()\n",
    "# Add the ground truth data to the left axis\n",
    "fig.add_trace(go.Scatter(y=df[location], x=df.index, name = location,  yaxis=\"y1\"))\n",
    "fig.add_trace(go.Scatter(y=df[location+'weather'], x=df.index, name = 'Temp',  yaxis=\"y2\"))\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(title=\"Time\"),\n",
    "    yaxis=dict(title=f\"Power\", side=\"left\"),\n",
    "    yaxis2=dict(title=\"Temperature [°C]\", overlaying=\"y\", side=\"right\"),\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## &#x2464; 'MFRED' Apartment data - United States of America\n",
    "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3690240.svg)](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/X9MIDJ)\n",
    "\n",
    "Used for: \n",
    "\n",
    "* 6_apartments\n",
    "\n",
    "----------\n",
    "* Duration: 1 year (Note: While this does not strictly meet the requirements, we have included this dataset due to its high quality. To achieve two years of data, we select two aparments that have an extremely similar yearly profile (measured by ) and concatenate these two as one timeseries)\n",
    "* Resolution: 15 minutes\n",
    "* Power Level: 0-3000 W"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Read in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join('data','raw_data','apartments_US','MFRED_Aggregates_15min_2019Q1-Q4.csv')\n",
    "\n",
    "spatial_scale = '6_apartment'\n",
    "unit_apartment = 'W'\n",
    "lat, lng = 40.776676, 73.971321 # New York City\n",
    "temp_resolutions = [5, 15, 60]\n",
    "\n",
    "df_ap = pd.read_csv(dir_path, index_col =0 , parse_dates = True).filter(regex=r'^.*kW$')*1000\n",
    "df_ap = df_ap.iloc[:, 1:] # exluding the aggregated load series\n",
    "df_ap.columns = [col.split('_')[0] for col in df_ap.columns] # renaming the columns since we converted to Watts above\n",
    "arconyms_apartment = {col: col.split(\"G\")[1] for col in df_ap.columns}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = df_ap.resample(\"4H\").mean()\n",
    "\n",
    "fig = px.line(df_plot)\n",
    "fig.update_layout(title='Apartment Power Demand', xaxis_title='Time', yaxis_title='Power (W)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special Processing: Concatenating similar apartment profiles\n",
    "\n",
    "As mentioned above, this dataset does not strictly meet the requirements. However, we have included this dataset due to its high measurement quality. To achieve two years of data (1 for training, the other for testing), we select two aparments that have a similar yearly profile (measured by [dynamic timewarping (DTW)](https://en.wikipedia.org/wiki/Dynamic_time_warping)) and concatenate these two as one timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to scale the data to cancel the effect of capacities (e.g. two apartments might have a similar behaviour but different appliances)\n",
    "scaler = MinMaxScaler()\n",
    "df_plot_scaled = pd.DataFrame(scaler.fit_transform(df_plot), columns=df_plot.columns, index=df_plot.index)\n",
    "df_plot_scaled = df_plot_scaled.fillna(method = \"bfill\", limit = 4).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will calculate the DTW on the df_plot dataframe which is resampled to 4 hours. This saves a lot of time,\n",
    "# while still preserving the intra-daily characteristics of the data\n",
    "\n",
    "dtw_matrix = dtw_distance_matrix(df_plot_scaled)\n",
    "\n",
    "# Then, get the upper triangle of the matrix (excluding the diagonal) to avoid duplicates\n",
    "dtw_upper_triangle = dtw_matrix.where(np.triu(np.ones(dtw_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Sort the values in ascending order and get the indices of the three smallest values\n",
    "smallest_indices = dtw_upper_triangle.unstack().sort_values().index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the column pairs with the smallest DTW distances\n",
    "for pair in smallest_indices[:8]:\n",
    "    print(f\"Columns '{pair[0]}' and '{pair[1]}' have a DTW distance of {dtw_matrix.loc[pair[0], pair[1]]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the two combinations with the smallest DTW distance\n",
    "for i in range(5):\n",
    "    fig = px.line(df_plot_scaled[list(smallest_indices[i])], title=f\"Apartment Power Demand {i}\")\n",
    "    fig.update_layout(xaxis_title='Time', yaxis_title='Power (W)')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ap_to_concat_1 = concat_and_scale(df_ap, smallest_indices[0]) # this is the pair with the smallest DTW distance\n",
    "df_ap_to_concat_2 = concat_and_scale(df_ap, smallest_indices[4]) # these are the two pairs with the second-smallest DTW distance that have no overlap with the one above\n",
    "df_ap_to_concat_3 = concat_and_scale(df_ap, smallest_indices[5]) # these are the two pairs with the second-smallest DTW distance that have no overlap with the one above"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the two pairs of apartments with the smallest DTW distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_ap_to_concat_1, title=f\"Apartment Power Demand 1\")\n",
    "fig.update_layout(xaxis_title='Time', yaxis_title='Power (W)')\n",
    "fig.show()\n",
    "\n",
    "fig_2 = px.line(df_ap_to_concat_2, title=f\"Apartment Power Demand 2\")\n",
    "fig_2.update_layout(xaxis_title='Time', yaxis_title='Power (W)')\n",
    "fig_2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apartments = pd.concat([df_ap_to_concat_1, df_ap_to_concat_2, df_ap_to_concat_3], axis=1)\n",
    "# as these are now hybrids of 6 apartments, we need to rename the columns\n",
    "df_apartments.columns = [f\"apartment_{i}\" for i in range(1, len(df_apartments.columns)+1)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The cleaning and saving (Like in the other spatial scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determined visually to meet criteria mentioned above\n",
    "train_begin = \"2019-01-01\"\n",
    "train_end = \"2020-01-01\"\n",
    "val_begin = \"2020-01-01\"\n",
    "val_end = \"2020-02-01\"\n",
    "test_begin = \"2020-08-01\"\n",
    "test_end = \"2020-09-01\"\n",
    "\n",
    "# 4_neighborhood\n",
    "spatial_scale = \"6_apartment\" # spatial scale of the data\n",
    "unit = \"W\" # unit of the demand data\n",
    "lat, lng = 40.776676, 73.971321 # New York City\n",
    "\n",
    "arconyms_neighborhood = {col: f'apartment_{i}' for i, col in enumerate(df_apartments.columns)}\n",
    "\n",
    "store = pd.HDFStore(os.path.join(save_path, f\"{spatial_scale}.h5\"), mode='w')\n",
    "for temp_resolution in temp_resolutions:\n",
    "    dfs = {}\n",
    "    for key, value in arconyms_neighborhood.items():\n",
    "        df = df_apartments[key].to_frame(f'{value}_demand_{unit}')\n",
    "        df = standardize_format(df, \"load\", temp_resolution, value, unit)\n",
    "        df = df.apply(lambda x: x.sub(x.groupby(x.index.date).transform(\"min\"))) # to avoid hovering above 0 for some profiles\n",
    "        df = remove_non_positive_values(df)\n",
    "        train, val, test = split_train_val_test_datasets(df, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "        dfs[value] = df\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_target', train, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_target', val, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_target', test, format='table')\n",
    "\n",
    "        start_date = df.index[0].strftime(\"%Y-%m-%d\")\n",
    "        end_date = df.index[-1].strftime(\"%Y-%m-%d\")\n",
    "        df_weather = get_weather_data(lat, lng, start_date, end_date, variables=['temperature_2m'], keep_UTC=False).tz_localize(None)\n",
    "        df_weather = standardize_format(df_weather, 'temperature', temp_resolution, value, \"C\")\n",
    "        df_weather = df_weather.reindex(df.index).dropna() \n",
    "        dfs[value + 'weather'] = df_weather\n",
    "        train_weather, val_weather, test_weather = split_train_val_test_datasets(df_weather, train_begin, train_end, val_begin, val_end, test_begin, test_end)\n",
    "\n",
    "        store.put(f'{value}/{temp_resolution}min/train_cov', train_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/val_cov', val_weather, format='table')\n",
    "        store.put(f'{value}/{temp_resolution}min/test_cov', test_weather, format='table')\n",
    "\n",
    "\n",
    "\n",
    "store.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'5_building.h5': ['building_0', 'building_1', 'building_2'],\n",
       "  '4_neighborhood.h5': ['neighborhood_0', 'neighborhood_1', 'neighborhood_2'],\n",
       "  '3_village.h5': ['village_0', 'village_1', 'village_2'],\n",
       "  '2_town.h5': ['town_0', 'town_1', 'town_2'],\n",
       "  '1_county.h5': ['Los_Angeles', 'New_York', 'Sacramento']},\n",
       " {'5_building.h5': ['60min'],\n",
       "  '4_neighborhood.h5': ['60min'],\n",
       "  '3_village.h5': ['15min', '60min'],\n",
       "  '2_town.h5': ['15min', '60min'],\n",
       "  '1_county.h5': ['60min']})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_hdf_keys(CLEAN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loading Data\n",
    "df_train = pd.read_hdf(\n",
    "    os.path.join(CLEAN_DATA_DIR, f\"1_county.h5\"),\n",
    "    key=f\"Los_Angeles/60min/train_target\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LearningBerkeley",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "32bd490ff79fd304cef95a146bac43660f32d9e1d5f3555df986ea8b6053493f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
