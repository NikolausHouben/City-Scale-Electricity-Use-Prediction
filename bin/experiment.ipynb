{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "from darts.metrics import smape, mape, mase, mse, rmse, r2_score, mae, max_peak_error, mean_n_peak_error\n",
    "\n",
    "from utils import *\n",
    "from train_eval import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Set seed\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnikolaushouben\u001b[0m (\u001b[33mwattcast\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\nik\\Desktop\\Berkeley_Projects\\WattCast\n"
     ]
    }
   ],
   "source": [
    "# Set working directory\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "os.chdir(r\"..\") # should be the git repo root directory, checking below:\n",
    "print(\"Current working directory: \" + os.getcwd())\n",
    "assert os.getcwd()[-8:] == \"WattCast\"\n",
    "dir_path = os.path.join(os.getcwd(), 'data', 'clean_data')\n",
    "model_dir = os.path.join(os.getcwd(), 'models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'1_county.h5': ['Los_Angeles', 'New_York', 'Sacramento'], '2_town.h5': ['town_0', 'town_1', 'town_2'], '3_village.h5': ['village_0', 'village_1', 'village_2'], '4_neighborhood.h5': ['germany'], '5_household.h5': ['household_0', 'household_1', 'household_2'], '6_apartment.h5': ['apartment_0', 'apartment_1', 'apartment_2']}, {'1_county.h5': ['60min'], '2_town.h5': ['15min', '60min'], '3_village.h5': ['15min', '60min'], '4_neighborhood.h5': ['15min', '60min'], '5_household.h5': ['15min', '60min'], '6_apartment.h5': ['15min', '5min', '60min']})\n"
     ]
    }
   ],
   "source": [
    "# See what keys are in the h5py data file\n",
    "print(get_hdf_keys(dir_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sorting runs by +summary_metrics.eval_loss\n"
     ]
    }
   ],
   "source": [
    "# run parameters\n",
    "tuned_models = [\n",
    "                #'rf',\n",
    "                'lgbm',\n",
    "                # 'xgb',\n",
    "                # 'gru',\n",
    "                # 'nbeats'\n",
    "                ]\n",
    "\n",
    "scale = '2_town'\n",
    "location = 'town_2'\n",
    "resolution = 60\n",
    "\n",
    "\n",
    "config_per_model = {}\n",
    "for model in tuned_models:\n",
    "    config, name = get_best_run_config('Wattcast_tuning', '+eval_loss', model, scale)\n",
    "    config['horizon_in_hours'] = 48 # we did finetuning for 24 hours, but we want to evaluate for 48 hours\n",
    "    config['location'] = location # we have to set the location explicitly, because we did not tune for it\n",
    "    config_per_model[model] = config, name\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca778f9ce8e41a9aa593868cc6804f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666899498, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\nik\\Desktop\\Berkeley_Projects\\WattCast\\wandb\\run-20230620_171506-2_town_town_2_60min</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/wattcast/Wattcast_2.0/runs/2_town_town_2_60min' target=\"_blank\">2_town_town_2_60min</a></strong> to <a href='https://wandb.ai/wattcast/Wattcast_2.0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/wattcast/Wattcast_2.0' target=\"_blank\">https://wandb.ai/wattcast/Wattcast_2.0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/wattcast/Wattcast_2.0/runs/2_town_town_2_60min' target=\"_blank\">https://wandb.ai/wattcast/Wattcast_2.0/runs/2_town_town_2_60min</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/wattcast/Wattcast_2.0/runs/2_town_town_2_60min?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x198f08df790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize wandb and log config\n",
    "#os.environ['WANDB_MODE'] = 'offline'\n",
    "\n",
    "name_id = scale + \"_\" + location + \"_\" + str(resolution) + \"min\"\n",
    "\n",
    "wandb.init(project=\"Wattcast_2.0\", name=name_id, id = name_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config().from_dict(config_per_model[tuned_models[0]][0])\n",
    "wandb.config.update(config.data)\n",
    "pipeline, ts_train_piped, ts_val_piped, ts_test_piped, ts_train_weather_piped, ts_val_weather_piped, ts_test_weather_piped, trg_train_inversed, trg_val_inversed, trg_test_inversed = data_pipeline(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting model instance for lgbm\n",
      "<train_eval.Config object at 0x00000198F084B2B0>\n",
      "getting model instance for linear regression\n"
     ]
    }
   ],
   "source": [
    "model_instances = get_model_instances(tuned_models, config_per_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_models, model_instances = load_trained_models(config, model_instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(model_instances) > 0:\n",
    "    just_trained_models, run_times = train_models(model_instances.values(), \n",
    "                            ts_train_piped,\n",
    "                            ts_train_weather_piped if config.weather else None, \n",
    "                            ts_val_piped,\n",
    "                            ts_val_weather_piped if config.weather else None,\n",
    "                            )\n",
    "    \n",
    "    df_runtimes = pd.DataFrame.from_dict(run_times, orient='index', columns=['runtime']).reset_index()\n",
    "    wandb.log({\"runtimes\": wandb.Table(dataframe=df_runtimes)})\n",
    "    trained_models.extend(just_trained_models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models_dict = {model.__class__.__name__: model for model in trained_models}\n",
    "model_names = list(models_dict.keys())\n",
    "save_models_to_disk(config, models_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sets = { # see data_prep.ipynb for the split\n",
    "            \"Winter\": (\n",
    "                        ts_val_piped[config.longest_ts_val_idx], \n",
    "                        None if not config.weather else ts_val_weather_piped[config.longest_ts_val_idx],\n",
    "                        trg_val_inversed\n",
    "                        ), \n",
    "            # \"Summer\": (\n",
    "            #             ts_test_piped[config.longest_ts_test_idx],\n",
    "            #             None if not config.weather else ts_test_weather_piped[config.longest_ts_test_idx],\n",
    "            #             trg_test_inversed\n",
    "            #             )\n",
    "                        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result_season = {}\n",
    "for season, (ts, ts_cov, gt) in test_sets.items():\n",
    "    print(f\"Testing on {season} data\")\n",
    "    # Generating Historical Forecasts for each model\n",
    "    ts_predictions_per_model = {}\n",
    "    historics_per_model = {}\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"Generating historical forecasts with {model_name}\")\n",
    "        historics = model.historical_forecasts(ts, \n",
    "                                            future_covariates= ts_cov if model.supports_future_covariates else None,\n",
    "                                            start=ts.get_index_at_point(config.n_lags),\n",
    "                                            verbose=True,\n",
    "                                            stride= 1, \n",
    "                                            forecast_horizon= config.timesteps_per_hour*48, \n",
    "                                            retrain=False, \n",
    "                                            last_points_only=False,\n",
    "                                            )\n",
    "        \n",
    "        \n",
    "        historics_inverted = [pipeline.inverse_transform(historic) for historic in historics][1:] # the first historic is partly nan, so we skip it\n",
    "        historics_per_model[model_name] = historics_inverted # storing the forecasts in batches of the forecasting horizon, for plot 2\n",
    "    \n",
    "    dict_result_season[season] = historics_per_model, gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_aheads = [i * config.timesteps_per_hour for i in [\n",
    "                                                    1, \n",
    "                                                    4, \n",
    "                                                    8, \n",
    "                                                    24, \n",
    "                                                    48\n",
    "                                                    ]] # horizon in hours\n",
    "\n",
    "\n",
    "dict_result_n_ahead = {}\n",
    "\n",
    "for n_ahead in n_aheads:\n",
    "    dict_result_season_update = {}\n",
    "    for season, (historics_per_model, gt) in dict_result_season.items():\n",
    "        ts_predictions_per_model = {}\n",
    "        historics_per_model_update = {}\n",
    "        for model_name, historics in historics_per_model.items():\n",
    "\n",
    "            ts_predictions = ts_list_concat_new(historics, n_ahead)\n",
    "            ts_predictions_per_model[model_name] = ts_predictions\n",
    "            historics_per_model_update[model_name] = historics\n",
    "\n",
    "        ts_predictions_per_model['24-Hour Persistence'] = gt.shift(config.timesteps_per_hour*24) # adding the 24-hour persistence model as a benchmark\n",
    "        dict_result_season_update[season] = historics_per_model_update, ts_predictions_per_model, gt\n",
    "    dict_result_n_ahead[n_ahead] = dict_result_season_update\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table & Plots"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_metrics = [rmse, r2_score, mae, smape, mape, max_peak_error] # evaluation metrics\n",
    "\n",
    "metrics_tables = []\n",
    "\n",
    "for n_ahead, dict_result_season in dict_result_n_ahead.items():\n",
    "    for season, (_, preds_per_model, gt) in dict_result_season.items():\n",
    "        df_metrics = get_error_metric_table(list_metrics, preds_per_model, gt)\n",
    "        rmse_persistence = df_metrics.loc[df_metrics.index == '24-Hour Persistence', 'rmse'].values[0]\n",
    "        df_metrics.drop(labels= [model_names[-1]], axis = 0, inplace=True)\n",
    "        df_metrics.reset_index(inplace=True)\n",
    "        df_metrics['season'] = season\n",
    "        df_metrics.set_index('season', inplace=True)\n",
    "        df_metrics.reset_index(inplace=True)\n",
    "        df_metrics['horizon_in_hours'] = n_ahead//config.timesteps_per_hour\n",
    "        df_metrics.set_index('horizon_in_hours', inplace=True)\n",
    "        df_metrics.reset_index(inplace=True)\n",
    "        df_metrics['rmse_skill_score'] = 1 - df_metrics['rmse'] / rmse_persistence\n",
    "        metrics_tables.append(df_metrics)\n",
    "\n",
    "df_metrics = pd.concat(metrics_tables, axis=0, ignore_index=True).sort_values(by=['season', 'horizon_in_hours'])\n",
    "\n",
    "wandb.log({f\"Error metrics for {config.spatial_scale} in {config.location}\": wandb.Table(dataframe=df_metrics)})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 1: Side-by-side comparison of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cov_train = pd.read_hdf(os.path.join(dir_path, f'{config.spatial_scale}.h5'), key=f'{config.location}/{config.temp_resolution}min/train_cov')\n",
    "df_cov_val = pd.read_hdf(os.path.join(dir_path, f'{config.spatial_scale}.h5'), key=f'{config.location}/{config.temp_resolution}min/val_cov')\n",
    "df_cov_test = pd.read_hdf(os.path.join(dir_path,f'{config.spatial_scale}.h5'), key=f'{config.location}/{config.temp_resolution}min/test_cov')\n",
    "temp_data = {'Summer': df_cov_test.iloc[:,0], 'Winter': df_cov_val.iloc[:,0]}\n",
    "\n",
    "\n",
    "for n_ahead, dict_result_season in dict_result_n_ahead.items():\n",
    "\n",
    "    for season, (_, preds_per_model, gt) in dict_result_season.items():\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add the ground truth data to the left axis\n",
    "        fig.add_trace(go.Scatter(x=gt.pd_series().index, y=gt.pd_series().values, name=\"Ground Truth\", yaxis=\"y1\"))\n",
    "\n",
    "        for model_name in model_names:\n",
    "            preds = preds_per_model[model_name]\n",
    "            fig.add_trace(go.Scatter(x=preds.pd_series().index, y=preds.pd_series().values, name=model_name, yaxis=\"y1\"))\n",
    "\n",
    "        # Add the df_cov_test data to the right axis\n",
    "        \n",
    "        series_weather = temp_data[season]\n",
    "        fig.add_trace(go.Scatter(\n",
    "        x=series_weather.index,\n",
    "        y=series_weather.values,\n",
    "        name=\"temperature\",\n",
    "        yaxis=\"y2\",\n",
    "        line=dict(dash=\"dot\", color = 'grey'),  # Set the line style to dotted\n",
    "    ))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title=f\"{season} - Horizon: {n_ahead// config.timesteps_per_hour} Hours\",\n",
    "            xaxis=dict(title=\"Time\"),\n",
    "            yaxis=dict(title=f\"Power [{ts_train_piped[0].components[0][-2:]}]\", side=\"left\"),\n",
    "            yaxis2=dict(title=\"Temperature [Â°C]\", overlaying=\"y\", side=\"right\"),\n",
    "        )\n",
    "\n",
    "        fig.show()\n",
    "        wandb.log({f\"{season} - Side-by-side comparison of predictions and the ground truth\": fig})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 2: Error Metric Trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_ahead, dict_result_season = list(dict_result_n_ahead.items())[-1]\n",
    "\n",
    "dict_result_season = dict_result_n_ahead[n_ahead]\n",
    "df_smapes_per_season = {}\n",
    "df_nrmse_per_season = {}\n",
    "\n",
    "for season, (historics_per_model, _, gt) in dict_result_season.items():\n",
    "    df_smapes_per_model = []\n",
    "    df_rmse_per_model = []\n",
    "    for model_name, historics in historics_per_model.items():\n",
    "        df_list = get_df_compares_list(historics, gt)\n",
    "        diffs = get_df_diffs(df_list)\n",
    "        df_smapes = abs(diffs).mean(axis =1) \n",
    "        df_smapes.columns = [model_name]\n",
    "        df_rmse = np.square(diffs).mean(axis =1) \n",
    "        df_rmse.columns = [model_name]\n",
    "\n",
    "        df_smapes_per_model.append(df_smapes)\n",
    "        df_rmse_per_model.append(df_rmse)\n",
    "\n",
    "    df_smapes_per_model = pd.concat(df_smapes_per_model, axis=1).ewm(alpha=0.1).mean()\n",
    "    df_smapes_per_model.columns = model_names\n",
    "    df_nrmse_per_model = pd.concat(df_rmse_per_model, axis=1).ewm(alpha=0.1).mean()\n",
    "    df_nrmse_per_model.columns = model_names\n",
    "    df_smapes_per_season[season] = df_smapes_per_model\n",
    "    df_nrmse_per_season[season] = df_nrmse_per_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for season in dict_result_season.keys():\n",
    "    fig = df_smapes_per_season[season].plot(figsize=(10,5))\n",
    "    plt.xlabel('Horizon')\n",
    "    plt.ylabel('MAPE [%]')\n",
    "    plt.legend(loc = 'upper left', ncol = 2)\n",
    "    plt.xticks(np.arange(0, n_ahead, 2))\n",
    "    plt.title(f\"Mean Absolute Percentage Error of the Historical Forecasts in {season}\")\n",
    "    wandb.log({f\"MAPE of the Historical Forecasts in {season}\": wandb.Image(fig)})\n",
    "    \n",
    "for season in dict_result_season.keys():\n",
    "    fig = df_nrmse_per_season[season].plot(figsize=(10,5))\n",
    "    plt.xlabel('Horizon')\n",
    "    plt.ylabel(f'RMSE [{ts_train_piped[0].components[0][-2:]}]')\n",
    "    plt.xticks(np.arange(0, n_ahead, 2))\n",
    "    plt.legend(loc = 'upper left', ncol = 2)\n",
    "    plt.title(f\"Root Mean Squared Error of the Historical Forecasts in {season}\")\n",
    "    wandb.log({f\"RMSE of the Historical Forecasts in {season}\": wandb.Image(fig)})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 3: Error distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season, (historics_per_model, _, gt) in dict_result_season.items():\n",
    "    df_smapes_per_model = []\n",
    "    df_nrmse_per_model = []\n",
    "    fig, ax = plt.subplots(ncols=len(model_names), figsize=(5*len(model_names),5))\n",
    "    fig.suptitle(f\"Error Distribution of the Historical Forecasts in {season}\")\n",
    "    for i, (model_name, historics) in enumerate(historics_per_model.items()):\n",
    "        df_list = get_df_compares_list(historics, gt)\n",
    "        diffs = get_df_diffs(df_list)\n",
    "        diffs_flat = pd.Series(diffs.values.reshape(-1,))\n",
    "        ax[i].hist(diffs_flat, bins=100)\n",
    "        ax[i].set_title(model_name)\n",
    "    \n",
    "    wandb.log({f\"Error Distribution of the Historical Forecasts in {season}\": wandb.Image(fig)})\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot 4: Per Day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for season, (_, preds_per_model, gt) in dict_result_season.items():\n",
    "    dfs_daily_sums = []\n",
    "    for model_name, preds in preds_per_model.items():\n",
    "        df_preds = preds.pd_series().to_frame(model_name + \"_preds\")\n",
    "        z = df_preds.groupby(df_preds.index.date).sum()\n",
    "        dfs_daily_sums.append(z)\n",
    "\n",
    "    df_gt = gt.pd_series().to_frame(\"ground_truth\") \n",
    "    z = df_gt.groupby(df_gt.index.date).sum() / config.timesteps_per_hour\n",
    "    dfs_daily_sums.append(z)\n",
    "    df_compare = pd.concat(dfs_daily_sums, axis=1).dropna()\n",
    "    fig = df_compare[:10].plot(kind='bar', figsize=(20,10))\n",
    "    plt.legend(loc = 'upper right', ncol = 2)\n",
    "    plt.ylabel(f'Energy [{ts_train_piped[0].components[0][-2:]}h]')\n",
    "    plt.title(f\"Daily Sum of the Predictions and the Ground Truth in {season}\")\n",
    "    wandb.log({f\"Daily Sum of the Predictions and the Ground Truth in {season}\": wandb.Image(fig)})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
